# server.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import os
import re
import chromadb
from sentence_transformers import SentenceTransformer
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from pyngrok import conf, ngrok

import nest_asyncio
import threading
import uvicorn
import sys
import time

#–¢–æ–∫–µ–Ω
NGROK_TOKEN = "39jhsX0Tw6Kp5vbEwB8xqZeaHbs_81R3cLX6c7bpau5HLpypn"
conf.get_default().auth_token = NGROK_TOKEN

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "knowledge_db")
print("SERVER DB_PATH =", DB_PATH)
print("SERVER CWD =", os.getcwd())


os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"

REFUSAL = "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."

# === Device ===
device = "cpu"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
if device == "cuda":
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# === Models ===
print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
embedder = SentenceTransformer("intfloat/multilingual-e5-small", device="cuda")

print("–ó–∞–≥—Ä—É–∑–∫–∞ LLM (Phi-3-mini)...")
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

if device == "cuda":
    model = AutoModelForCausalLM.from_pretrained(
        "microsoft/Phi-3-mini-4k-instruct",
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
    )
else:
    model = AutoModelForCausalLM.from_pretrained(
        "microsoft/Phi-3-mini-4k-instruct",
        trust_remote_code=True,
        torch_dtype=torch.float32,
    ).to("cpu")

model.eval()

# === Chroma ===
print("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π...")
#client = chromadb.PersistentClient(path="knowledge_db")
client = chromadb.PersistentClient(path=DB_PATH)
collection = client.get_collection("satellites")

# === API ===
app = FastAPI(title="CosmoVision AI Backend", version="2.1")


class QueryRequest(BaseModel):
    text: str


# ---------- Utils ----------
def normalize_query(q: str) -> str:
    """–ú–∏–Ω–∏-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –ø—Ä–∏–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä –∏ —á–∞—Å—Ç—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏."""
    q = (q or "").strip()
    q = re.sub(r"\s+", " ", q)
    # —Ç–≤–æ–π –∫–µ–π—Å: –ú–µ—Ç—Ä–µ–æ—Ä -> –ú–µ—Ç–µ–æ—Ä
    q = re.sub(r"–º–µ—Ç—Ä–µ–æ—Ä", "–º–µ—Ç–µ–æ—Ä", q, flags=re.IGNORECASE)
    return q


def is_off_topic(query: str) -> bool:
    q = query.lower()

    personal = [
        "—Ç—ã –∫—Ç–æ", "–∫—Ç–æ —Ç—ã", "–∫–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç",
        "–ø—Ä–∏–≤–µ—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", "–∫–∞–∫ –¥–µ–ª–∞",
        "—á—Ç–æ —Ç—ã", "—Ç—ã —Ä–æ–±–æ—Ç", "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "–ø–æ–º–æ—â–Ω–∏–∫"
    ]
    if any(p in q for p in personal):
        return True

    on_topic = [
        "—Å–ø—É—Ç–Ω–∏–∫", "–∫–æ—Å–º–æ—Å", "–∫–æ—Å–º–æ–Ω–∞–≤—Ç–∏–∫–∞", "–æ—Ä–±–∏—Ç–∞", "–∞–Ω—Ç–µ–Ω–Ω–∞",
        "–ø–∞–Ω–µ–ª", "–º–µ—Ç–µ–æ—Ä", "–≥–ª–æ–Ω–∞—Å—Å", "–∞–ø–ø–∞—Ä–∞—Ç", "—Ä–∞–∫–µ—Ç–∞", "–∑–∞–ø—É—Å–∫",
        "—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", "—Ä–∞–±–æ—Ç–∞–µ—Ç", "–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ", "—Å–∏—Å—Ç–µ–º–∞", "–º–æ–¥—É–ª—å",
        "–¥–≤–∏–≥–∞—Ç–µ–ª—å", "—Å–æ–ª–Ω–µ—á–Ω", "–ø–∏—Ç–∞–Ω–∏–µ", "–ø–µ—Ä–µ–¥–∞—á", "—Å–≤—è–∑—å",
        "–¥–∞—Ç—á–∏–∫", "–∫–∞–º–µ—Ä–∞", "—Ç–µ–ª–µ–º–µ—Ç—Ä",
    ]
    return not any(k in q for k in on_topic)


def clean_doc_keep_header(text: str) -> str:
    """
    –í–ê–ñ–ù–û: –ù–ï —É–±–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏.
    - —É–±–∏—Ä–∞–µ–º "passage:"
    - '# –ó–∞–≥–æ–ª–æ–≤–æ–∫' –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ '–ó–∞–≥–æ–ª–æ–≤–æ–∫.'
    - –≤—ã–∫–∏–¥—ã–≤–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    """
    text = (text or "").replace("passage: ", "").strip()
    if not text:
        return ""

    out = []
    for line in text.splitlines():
        s = line.strip()
        if not s:
            continue
        if s.startswith("#"):
            # –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º '# –ú–µ—Ç–µ–æ—Ä-–ú' -> '–ú–µ—Ç–µ–æ—Ä-–ú.'
            hdr = s.lstrip("#").strip()
            if hdr:
                if not hdr.endswith("."):
                    hdr += "."
                out.append(hdr)
            continue
        out.append(s)

    return "\n".join(out).strip()


def retrieve_context(query: str, initial_n: int = 3, max_n: int = 8) -> tuple[str, list[float] | None]:
    """
    –î–æ—Å—Ç–∞—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω –±—ã–ª –ù–ï –ø—É—Å—Ç–æ–π –∏ –ù–ï –æ–≥—Ä—ã–∑–æ–∫.
    –ï—Å–ª–∏ –ø–µ—Ä–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—É—Å—Ç—ã–µ/–∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—Å–ª–µ —á–∏—Å—Ç–∫–∏ ‚Äî —Ä–∞—Å—à–∏—Ä—è–µ–º n_results.
    """
    q_emb = embedder.encode([f"query: {query}"], normalize_embeddings=True)

    n = initial_n
    best_distances = None
    best_context = ""

    while n <= max_n:
        try:
            results = collection.query(
                query_embeddings=q_emb,
                n_results=n,
                include=["documents", "distances"],
            )
            distances = results.get("distances", None)
        except TypeError:
            results = collection.query(query_embeddings=q_emb, n_results=n)
            distances = None

        docs = (results.get("documents") or [[]])[0]
        cleaned = []
        for d in docs:
            cd = clean_doc_keep_header(d)
            if cd:
                cleaned.append(cd)

        context = "\n\n".join(cleaned).strip()

        # distances —Ñ–æ—Ä–º–∞—Ç –æ–±—ã—á–Ω–æ [[...]]
        dist_list = None
        if isinstance(distances, list) and distances and isinstance(distances[0], list):
            dist_list = distances[0]

        # –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç (—Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –Ω–µ–ø—É—Å—Ç–æ–π)
        if context and len(context) > len(best_context):
            best_context = context
            best_distances = dist_list

        # –µ—Å–ª–∏ —É–∂–µ –Ω–æ—Ä–º ‚Äî –≤—ã—Ö–æ–¥–∏–º
        if len(context) >= 80:
            return context, dist_list

        # –∏–Ω–∞—á–µ —Ä–∞—Å—à–∏—Ä—è–µ–º –≤—ã–±–æ—Ä–∫—É
        n += 2

    return best_context, best_distances


def looks_answerable(query: str, context: str) -> bool:
    if not context.strip():
        return False

    q = query.lower()
    c = context.lower()

    definitional = any(x in q for x in ["—á—Ç–æ —Ç–∞–∫–æ–µ", "—á—Ç–æ –∑–Ω–∞—á–∏—Ç", "–æ–ø—Ä–µ–¥–µ–ª–∏", "–¥–∞—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"])

    if definitional:
        # –î–ª—è "—á—Ç–æ —Ç–∞–∫–æ–µ —Å–ø—É—Ç–Ω–∏–∫ –ú–µ—Ç–µ–æ—Ä-–ú" –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á—Ç–æ–±—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –±—ã–ª–∞ —Å—Ç—Ä–æ–∫–∞ —Å "‚Äî —ç—Ç–æ"
        # –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ ("–º–µ—Ç–µ–æ—Ä")
        markers = ["‚Äî —ç—Ç–æ", "—ç—Ç–æ ", "–ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á", "–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è", "—Å–ª—É–∂–∏—Ç –¥–ª—è", "–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π"]
        has_marker = any(m in c for m in markers)

        # –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –ú–µ—Ç–µ–æ—Ä ‚Äî —Ö–æ—Ç–∏–º, —á—Ç–æ–±—ã —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å
        if "–º–µ—Ç–µ–æ—Ä" in q:
            return has_marker and ("–º–µ—Ç–µ–æ—Ä" in c)
        return has_marker

    # –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: —Ö–æ—Ç—è –±—ã –∫–∞–∫–æ–π-—Ç–æ –æ–±—ä—ë–º
    return len(context) >= 40


def extract_definition_from_context(query: str, context: str) -> str | None:
    q = query.lower()
    c = context

    # –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –ú–µ—Ç–µ–æ—Ä ‚Äî –∏—â–µ–º —Å—Ç—Ä–æ–∫—É "–ú–µ—Ç–µ–æ—Ä-–ú ‚Äî —ç—Ç–æ ..."
    if "–º–µ—Ç–µ–æ—Ä" in q:
        m = re.search(r"(–ú–µ—Ç–µ–æ—Ä[-‚Äì‚Äî ]?–ú\s*‚Äî\s*—ç—Ç–æ[^\n\.]*[\.]?)", c, flags=re.IGNORECASE)
        if m:
            return m.group(1).strip()

    # –æ–±—â–∏–π —Å–ª—É—á–∞–π: –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å "‚Äî —ç—Ç–æ"
    m = re.search(r"^(.{0,80}?‚Äî\s*—ç—Ç–æ[^\n\.]*[\.]?)", c, flags=re.IGNORECASE | re.MULTILINE)
    if m:
        return m.group(1).strip()

    return None

def is_definitional(q: str) -> bool:
    ql = q.lower()
    return any(x in ql for x in ["—á—Ç–æ —Ç–∞–∫–æ–µ", "—á—Ç–æ –∑–Ω–∞—á–∏—Ç", "–æ–ø—Ä–µ–¥–µ–ª–∏", "–¥–∞—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"])


def generate_answer_strict(query: str, context: str) -> str:
    # 1) –µ—Å–ª–∏ –º–æ–∂–µ–º –æ—Ç–≤–µ—Ç–∏—Ç—å –±–µ–∑ LLM ‚Äî –æ—Ç–≤–µ—á–∞–µ–º –±–µ–∑ LLM (0 –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π)
    if is_definitional(query):
        defin = extract_definition_from_context(query, context)
        if defin:
            return defin
        return REFUSAL

    # 2) –¥–ª—è –Ω–µ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π ‚Äî LLM, –Ω–æ –ë–ï–ó FACT/ANSWER —Ñ–æ—Ä–º–∞—Ç–∞
    if not context.strip():
        return REFUSAL

    system = (
        "–¢—ã ‚Äî –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –∫–æ—Å–º–æ–Ω–∞–≤—Ç–∏–∫–µ.\n"
        "–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É. –ù–µ–ª—å–∑—è –¥–æ–±–∞–≤–ª—è—Ç—å —Ñ–∞–∫—Ç—ã –Ω–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\n"
        f"–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞, –≤–µ—Ä–Ω–∏ —Ä–æ–≤–Ω–æ: {REFUSAL}"
    )

    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": f"–ö–û–ù–¢–ï–ö–°–¢:\n{context}\n\n–í–û–ü–†–û–°: {query}"},
    ]

    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=3800).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=120,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    input_len = inputs.input_ids.shape[1]
    text = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True).strip()

    # –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ—Å—ë—Ç —á—Ç–æ-—Ç–æ —Å–æ–≤—Å–µ–º –ª–µ–≤–æ–µ ‚Äî —Ö–æ—Ç—è –±—ã –æ—Ç—Ä–µ–∂–µ–º –∏ –ø–æ–¥—Å—Ç—Ä–∞—Ö—É–µ–º
    if not text or len(text) < 3:
        return REFUSAL
    return text


# ---------- Endpoint ----------
@app.post("/query")
def handle_query(req: QueryRequest):
    try:
        raw = req.text or ""
        q = normalize_query(raw)

        if not q:
            return {"query": raw, "answer": REFUSAL, "context_used": False}

        if is_off_topic(q):
            return {
                "query": q,
                "answer": (
                    "–Ø –æ—Ç–≤–µ—á–∞—é —Ç–æ–ª—å–∫–æ –ø–æ –∫–æ—Å–º–æ–Ω–∞–≤—Ç–∏–∫–µ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π. "
                    "–°–ø—Ä–æ—Å–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω—ã —Å–æ–ª–Ω–µ—á–Ω—ã–µ –ø–∞–Ω–µ–ª–∏ –Ω–∞ –ú–µ—Ç–µ–æ—Ä–µ-–ú?¬ª"
                ),
                "context_used": False,
            }

        context, distances = retrieve_context(q, initial_n=3, max_n=9)

        # –ú—è–≥–∫–∏–π –æ—Ç—Å–µ–≤ –ø–æ distance (–µ—Å–ª–∏ –µ—Å—Ç—å). –ü–æ—Ä–æ–≥ –ø—É—Å—Ç—å –±—É–¥–µ—Ç –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π.
        if distances is not None and len(distances) > 0:
            best = distances[0]
            if best is not None and best > 0.9:
                return {"query": q, "answer": REFUSAL, "context_used": False}

        answer = generate_answer_strict(q, context)

        return {
            "query": q,
            "answer": answer,
            "context_used": (answer != REFUSAL),
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")


@app.get("/debug/kb")
def debug_kb():
    return {
        "db_path": DB_PATH,
        "cwd": os.getcwd(),
        "count": collection.count(),
    }

@app.post("/debug/search")
def debug_search(req: QueryRequest):
    q = normalize_query(req.text)

    q_emb = embedder.encode([f"query: {q}"], normalize_embeddings=True)

    try:
        res = collection.query(
            query_embeddings=q_emb,
            n_results=5,
            include=["documents", "distances", "metadatas"],
        )
    except TypeError:
        res = collection.query(query_embeddings=q_emb, n_results=5)

    docs = (res.get("documents") or [[]])[0]
    dists = (res.get("distances") or [[]])[0]
    metas = (res.get("metadatas") or [[]])[0]

    n = max(len(docs), len(dists), len(metas))

    top = []
    for i in range(n):
        top.append({
            "i": i,
            "distance": dists[i] if i < len(dists) else None,
            "meta": metas[i] if i < len(metas) else None,
            "doc": docs[i] if i < len(docs) else None,
        })

    return {
        "query": q,
        "server_cwd": os.getcwd(),
        "db_path": DB_PATH if "DB_PATH" in globals() else "unknown",
        "collection_count": collection.count(),
        "lens": {"docs": len(docs), "dists": len(dists), "metas": len(metas)},
        "top": top,
    }


# === Warmup ===
print("–ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏...")
try:
    test_context = "–°–ø—É—Ç–Ω–∏–∫ ‚Äî —ç—Ç–æ –∞–ø–ø–∞—Ä–∞—Ç, –æ–±—Ä–∞—â–∞—é—â–∏–π—Å—è –≤–æ–∫—Ä—É–≥ –ó–µ–º–ª–∏."
    _ = generate_answer_strict("–ß—Ç–æ —Ç–∞–∫–æ–µ —Å–ø—É—Ç–Ω–∏–∫?", test_context)
    print("‚úÖ –°–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤!")
except Exception as e:
    print(f"‚ö†Ô∏è –ü—Ä–æ–≥—Ä–µ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º: {e}")



nest_asyncio.apply()
sys.path.insert(0, os.getcwd())

def run_server():
    uvicorn.run(app, host="0.0.0.0", port=8000)

thread = threading.Thread(target=run_server, daemon=True)
thread.start()

time.sleep(3)
print("\nüåê –ü—Ä–æ–±—Ä–æ—Å –ø–æ—Ä—Ç–∞ —á–µ—Ä–µ–∑ ngrok...")
try:
    public_url = ngrok.connect(8000)
    NGROK_URL = str(public_url)
    
    print("\n" + "="*60)
    print("üöÄ –°–ï–†–í–ï–† –†–ê–ë–û–¢–ê–ï–¢!")
    print("="*60)
    print(f"üîó Swagger UI: {NGROK_URL}/docs")
    print(f"üîó Debug: {NGROK_URL}/debug/kb")
    print("="*60)
    
    # –ì–õ–ê–í–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ù–µ –¥–∞–µ–º —Å–∫—Ä–∏–ø—Ç—É –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è
    print("–û–∂–∏–¥–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π (–Ω–µ –∑–∞–∫—Ä—ã–≤–∞–π—Ç–µ —ç—Ç—É —è—á–µ–π–∫—É)...\n")
    while True:
        time.sleep(1)
        
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ ngrok: {e}")
    # –ï—Å–ª–∏ ngrok —É–ø–∞–¥–µ—Ç, –≤—Å–µ —Ä–∞–≤–Ω–æ –¥–µ—Ä–∂–∏–º –ø—Ä–æ—Ü–µ—Å—Å –∂–∏–≤—ã–º, —á—Ç–æ–±—ã Colab –Ω–µ —É–±–∏–ª —è—á–µ–π–∫—É
    while True:
        time.sleep(1)

